{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepFool Algorithm\n",
    "\n",
    "- **Authors:** Guilherme Magalh√£es, Jo√£o Sousa, Jo√£o Baptista\n",
    "- **University:** Faculty of Sciences of University of Porto\n",
    "- **Course:** Machine Learning II (CC3043)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a given classifier and example, the algorithm is set to compute the minimal perturbation that is sufficient to change the estimated label. For the remainder of the section, the following notation will be used:\n",
    "\n",
    "- $f$: a given classifier that outputs a vector with the probability distribution for the classification associated with its probability index.\n",
    "\n",
    "- $x$: a given example.\n",
    "\n",
    "- $X$: the domain of the examples.\n",
    "\n",
    "- $T$: the domain of test examples available.\n",
    "\n",
    "- $k$: a possible classification of the considered problem. Thus, the probability of the classification of an example $x$ to be $k$ is $f_k(x)$.\n",
    "\n",
    "- $\\hat{k}(x)$: the estimated classification of a given example. It is noted that $\\hat{k}(x) = argmax_k( f_k(x) )$.\n",
    "\n",
    "- $\\hat{r}(x)$: the minimal perturbation for which $\\hat{k}(x) \\ne \\hat{k}(x+\\hat{r}(x))$.\n",
    "\n",
    "The DeepFool algorithm only outputs the value of the minimal perturbation $\\hat{r}(x)$ of a given example $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the proposed formal definition for the robustness to adversarial examples of a given classifier is the expected value over the domain of examples for the norm of the minimal perturbation for an example divided by the norm of that same example. For practical purposes, the aforementioned expected value is approximated to the mean value for all examples in the available test domain of the classifier:\n",
    "\n",
    "$\\rho_{adv}(f) = ùîº_X \\frac{||\\hat{r}(x)||_2}{||x||_2} ‚âà \\frac{1}{|T|} ‚àë_{x\\in T} \\frac{||\\hat{r}(x)||_2}{||x||_2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Necessary Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import librosa\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import keras\n",
    "import gc\n",
    "from tensorflow.keras import optimizers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pkl(data, path):\n",
    "    try:\n",
    "        with open(path, \"wb\") as saved_data:\n",
    "            pickle.dump(data, saved_data)\n",
    "        saved_data.close()\n",
    "    except:\n",
    "        print('Fail to save data')\n",
    "\n",
    "def load_pkl(path):\n",
    "    try:\n",
    "        with open(path, \"rb\") as loaded_data:\n",
    "            to_return = pickle.load(loaded_data)\n",
    "        loaded_data.close()\n",
    "        return to_return\n",
    "    except:\n",
    "        print('Fail to load data')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of the Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient of class k with respect to the input\n",
    "def get_gradient(model, x, k):\n",
    "    x = tf.convert_to_tensor(x, dtype=tf.float32)\n",
    "\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        tape.watch(x)\n",
    "        # model's output\n",
    "        results = model(x)\n",
    "        results_k = results[0, k]  # k-th class output for the first input in the batch\n",
    "    gradients = tape.gradient(results_k, x)  \n",
    "\n",
    "    del tape\n",
    "    return gradients.numpy(), results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate robustness of an individual example \n",
    "def example_robustness(x, r):\n",
    "    r_norm = np.sqrt(np.sum([np.linalg.norm(r_input)**2 for r_input in r]))\n",
    "    x_norm = np.sqrt(np.sum([np.linalg.norm(x_input)**2 for x_input in x]))\n",
    "    return r_norm / x_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model: the model to be used in the algorithm\n",
    "# x0: the initial input without any perturbation\n",
    "# eta: an overshoot value to be multiplied with each iteration's perturbation\n",
    "# max_iter: maximum nummber of iterations  the algorithm is allowed to execute\n",
    "def deepfool(model, x0, eta=0.01, max_iter=20):\n",
    "\n",
    "    # obtain the initial estimated label\n",
    "    f_x0 = model.predict(x0).flatten()\n",
    "    print(f\"f_x0 = {f_x0}\")\n",
    "    label_x0 = f_x0.argsort()[::-1][0]\n",
    "\n",
    "    loop_i = 0\n",
    "    xi = deepcopy(x0)\n",
    "    label_xi = label_x0\n",
    "    r = []\n",
    "\n",
    "    # main loop\n",
    "    while label_xi == label_x0 and loop_i < max_iter:\n",
    "        w_l = [np.zeros(x_input.shape) for x_input in x0]\n",
    "        f_l = 0\n",
    "        fk_wk_min = np.inf\n",
    "        grad_f_label_x0_on_xi, f_xi = get_gradient(model, xi, label_x0)\n",
    "\n",
    "        for k in range(10): # k = 0, ..., 9 (possible classes in the problem considered for this project)\n",
    "            if (k == label_x0):\n",
    "                continue\n",
    "            grad_f_k_on_xi, f_xi = get_gradient(model, xi, k)\n",
    "            w_k = [g_f_k - g_f_label for g_f_k, g_f_label in zip(grad_f_k_on_xi, grad_f_label_x0_on_xi)]\n",
    "            w_k_norm = np.sqrt(np.sum(np.fromiter([np.linalg.norm(w_k_input)**2 for w_k_input in w_k], dtype=np.float32)))\n",
    "            f_k = f_xi[0,k] - f_xi[0,label_x0]\n",
    "            fk_wk = np.linalg.norm(f_k) / (w_k_norm + 1e-3)\n",
    "            if fk_wk < fk_wk_min:\n",
    "                w_l, f_l = w_k, f_k\n",
    "        \n",
    "        w_l_squared_norm = np.sum(np.fromiter([np.linalg.norm(w_l_input)**2 for w_l_input in w_l], dtype=np.float32))\n",
    "        f_l_norm = np.linalg.norm(f_l)\n",
    "        ri_const = f_l_norm / (w_l_squared_norm + 1e-3)\n",
    "        ri = [ri_const * w_l_input for w_l_input in w_l]\n",
    "        r.append(ri)\n",
    "        xi_new = [xi_item + (1+eta)*ri_item for xi_item, ri_item in zip(xi, ri)]\n",
    "        xi = xi_new\n",
    "        xi = np.array(xi)\n",
    "        print(f\"xi shape: {xi.shape}\")\n",
    "        label_xi = model.predict(xi).flatten()\n",
    "        label_xi = label_xi.argsort()[::-1][0]\n",
    "        loop_i += 1\n",
    "\n",
    "    # main loop finished\n",
    "    r_sum = [np.zeros(x_input.shape) for x_input in x0]\n",
    "    for i in range(len(x0)):\n",
    "        for r_i in r:\n",
    "            r_sum[i] += r_i[i][0]\n",
    "\n",
    "    # return the value of r(x), number of iterations performed, and the new label obtained by adding the perturbation to the input\n",
    "    return r_sum, loop_i, label_xi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running DeepFool for the RNN-LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Robustness for a classifier is practically defined as the mean value of the norm of the minimal perturbation required for an example, divided by the norm of the example itself.\n",
    "\n",
    "Since the RNN developed in this project was evaluated using a 10-fold cross-validation approach, it was necessary to calculate the robustness for each model trained during the cross-validation process.\n",
    "\n",
    "For each iteration, the test data used to assess robustness to adversarial examples was the same as the test data used to compute the cross-validation metrics in the notebook rnn-lstm.ipynb.\n",
    "\n",
    "The final robustness results are presented as the mean and standard deviation of the values obtained across all cross-validation test datasets. These results can be found in rnn-lstm.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = load_pkl(\"data/features_rnn.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in range(1, 10+1):\n",
    "\n",
    "    # this list will hold the values ||r(x)|| / ||x|| initially mentioned on the notebook \n",
    "    robustness_values_rnn_fold = []\n",
    "\n",
    "    # utilize only the data of the f-th fold\n",
    "    X_fold = df_data[f]\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    # load the model from the performance evaluation cross validation run that used the f-th fold as the test fold\n",
    "    # the compile keyword argument was set to false such that the model can be explicitly re-compiled using the compile settings defined on LSTM.ipynb\n",
    "    fold_model_rnn = keras.models.load_model(f\"kfold_metrics_LSTM/model_fold{f}.keras\", compile=False)\n",
    "    fold_model_rnn.compile(\n",
    "        optimizer=optimizers.Adam(learning_rate=0.001),\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    # begin the run of each example in the f-th fold and the corresponding model on the DeepFool algorithm\n",
    "    for i in range(len(X_fold)):\n",
    "        print(f\"Shape of X_fold[{i}]: {X_fold[i].shape}\")  # Debugging\n",
    "    \n",
    "        example_input = np.expand_dims(X_fold[i], axis = 0)\n",
    "        print(f\"Shape of example_input: {example_input.shape}\")\n",
    "\n",
    "        # prepare the input for the implemented DeepFool function\n",
    "        \n",
    "\n",
    "        # run DeepFool\n",
    "        # eta keyword argument was set to 10^6 due to the extremely small order of magnitude of each iteration's perturbation (between 10^-8 and 10^-5)\n",
    "        perturbation, iters, fool_label = deepfool(fold_model_rnn, example_input, eta=1e6)\n",
    "\n",
    "        # save the ||r(x)|| / ||x|| value into the list defined in the beginning of the \"fold\" for loop\n",
    "        robustness_values_rnn_fold.append(example_robustness(example_input, perturbation))\n",
    "\n",
    "    del X_fold\n",
    "    gc.collect()\n",
    "\n",
    "    # to save the results of each fold\n",
    "    save_pkl(robustness_values_rnn_fold, f\"robustness/robustness_values_rnn_fold{f}.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Considerations\n",
    "\n",
    "Considering that the deepfool algorithm took a long time to run, we were only able to evaluate the robustness of the RNN-LSTM model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- Moosavi-Dezfooli, S.-M., Fawzi, A., Frossard, P., Polytechnique, E. and De Lausanne, F. (2016). DeepFool: a simple and accurate method to fool deep neural networks: https://openaccess.thecvf.com/content_cvpr_2016/papers/Moosavi-Dezfooli_DeepFool_A_Simple_CVPR_2016_paper.pdf.\n",
    "\n",
    "- TensorFlow. (2023). Introduction to gradients and automatic differentiation | TensorFlow Core. [online] Available at: https://www.tensorflow.org/guide/autodiff."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
